# Gradient Descent 기본 개념

> Type: Concept

## 상황/배경
모델 학습에서 “파라미터를 어떻게 업데이트해서 성능을 좋아지게 할까?”라는 질문이 생긴다.  
이때 가장 기본이 되는 최적화(optimization) 방법이 Gradient Descent(GD, 경사 하강법)이다.

## 정의
Gradient Descent는 **손실 함수(loss function)의 값을 최소로 만드는 파라미터(weight, bias 등)를 찾기 위해**,  
손실 함수를 파라미터로 미분한 **기울기(gradient)** 를 사용해 **반복적으로 파라미터를 업데이트하는 방법**이다.

## 핵심 아이디어
- 손실 함수는 고정된 “함수”이고, 줄어드는 대상은 **손실 함수의 값**이다.
- 파라미터에서 손실 함수의 기울기를 구하면, 그 방향이 **손실 값을 가장 빨리 증가시키는 방향**을 알려준다.
- 따라서 **기울기의 반대 방향**으로 조금씩 이동하면 손실 값이 감소한다.

## 수식으로 보기
파라미터를 $\theta$라고 하면, GD 업데이트는 다음과 같다.

$$
\theta \leftarrow \theta - \eta \nabla_\theta J(\theta)
$$

- $J(\theta)$: 손실 함수의 값
- $\nabla_\theta J(\theta)$: 파라미터에 대한 손실 함수의 기울기
- $\eta$ (learning rate): 학습률, 한 번에 이동하는 크기

## GD 한 스텝(Iteration) 흐름
1. 현재 파라미터로 예측값 계산
2. 손실 함수 값 계산
3. 손실 함수를 파라미터로 미분해 gradient 계산
4. gradient의 **반대 방향**으로 파라미터 업데이트

즉, **“GD를 한 번 수행한다 = gradient를 계산하고 파라미터를 업데이트한다”**로 이해하면 된다.

## 자주 쓰는 표현 정리
- “손실함수를 최소화한다”  
  → 관용적 축약 표현. 엄밀히는 **“손실 함수의 값을 최소화한다”**가 정확하다.
- “GD 계산한다”  
  → 보통 **GD 스텝(업데이트)을 수행한다**는 의미로 쓰인다.

## 주의할 점
- **손실 값 자체**는 모니터링용 스칼라 값이고, 업데이트에 직접 들어가는 것은 **gradient**이다.
- 학습률 $\eta$가 너무 크면 발산하고, 너무 작으면 수렴이 느리다.

## 한 줄 요약
GD는 **손실 함수의 gradient로 파라미터를 반복 업데이트**해  
손실 함수 **값이 최소가 되는 파라미터를 찾는 최적화 방법**이다.
