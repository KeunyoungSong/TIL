# Conv2D 커널 값(가중치) 업데이트: 왜 “여러 위치의 합”이 되나

> Type: Concept

## 상황/배경(Context)
CNN에서 `Conv2D`의 filter(kernel)도 학습된다는 게 처음엔 낯설다.  
특히 “weight sharing 때문에 gradient가 여러 위치에서 온 신호의 합으로 누적된다”는 말을 직관/수식으로 한 번에 정리해둔다.

---

## 정의(Definition)
`Conv2D`의 **kernel(filter)**은 학습 가능한 파라미터(가중치)이고, 학습은 다른 가중치(Dense 등)와 동일하게 **backprop으로 gradient를 계산하고 optimizer로 업데이트**하는 과정이다.

## 핵심 아이디어(Key Ideas)
- `kernel`의 각 칸 값은 전부 **학습되는 weight**다.
- Conv는 같은 kernel을 이미지의 모든 위치에 반복 적용한다(= **weight sharing**).
- 그래서 kernel 한 칸에 대한 gradient는 “어떤 한 위치”의 기여가 아니라, **모든 위치(그리고 배치의 모든 샘플)에서 나온 기여가 합산**된 값이 된다.

## 기호 정리(Symbols)
- $X$: 입력 텐서
- $W$: kernel(weight)
- $Y$: Conv 출력(feature map)
- $L$: loss
- $n$: batch index
- $o$: output channel index
- $c$: input channel index
- $(i, j)$: 출력의 spatial 위치
- $(u, v)$: kernel의 spatial 위치

## 도식(Diagram)
```
같은 kernel W를...
  (1) 왼쪽 위 패치에 적용
  (2) 한 칸 옮겨 다음 패치에 적용
  (3) ... 끝까지 반복

=> W는 "위치마다 다른 W"가 아니라, "모든 위치가 같은 W"를 공유
=> 그래서 dL/dW는 모든 위치의 기여가 누적(합)
```

## 원리/수식(Principle/Math)
프레임워크마다 표기가 조금 다르지만(많은 구현은 convolution이라기보다 cross-correlation 형태),
핵심 구조는 동일하다.

### Forward (개념식)
$$
Y_{n,o,i,j}=\sum_{c}\sum_{u}\sum_{v} W_{o,c,u,v}\,X_{n,c,i+u,j+v} + b_o
$$

### Backward: kernel 한 칸의 gradient는 “합”
kernel의 특정 칸 $W_{o,c,u,v}$에 대한 gradient는 다음처럼 쓸 수 있다.

$$
\frac{\partial L}{\partial W_{o,c,u,v}}
=
\sum_{n}\sum_{i}\sum_{j}
\frac{\partial L}{\partial Y_{n,o,i,j}}\; X_{n,c,i+u,j+v}
$$

여기서 중요한 포인트는 $\sum_{n}\sum_{i}\sum_{j}$:
- 같은 $W_{o,c,u,v}$가 **배치의 모든 샘플($n$)**, **출력의 모든 위치($i,j$)**에서 재사용되기 때문에
- 그 위치들에서 나온 “오차 신호” $\frac{\partial L}{\partial Y}$가 **한 커널 값의 업데이트량으로 누적**된다.

## 예시(Examples)
### 1) “한 칸의 weight”가 여러 위치에서 동시에 업데이트되는 느낌
`3×3 kernel`의 왼쪽 위 칸 $W[\text{u}=0,\text{v}=0]$을 생각해보면,
출력의 각 위치 $(i,j)$에서 이 weight가 곱해지는 입력 값은 $X[i+0, j+0]$로 매번 달라진다.

즉 같은 weight 하나가:
- (i=0,j=0)에서는 입력의 (0,0) 픽셀과 곱해지고
- (i=0,j=1)에서는 입력의 (0,1) 픽셀과 곱해지고
- ...

그래서 학습 시에는 각 위치에서의 `upstream gradient * 해당 입력값`이 모두 더해져서 그 weight를 움직인다.

### 2) Update는 다른 가중치와 동일
SGD라면 커널 값 업데이트는 다음처럼 “그냥 가중치 업데이트”다.

$$
W \leftarrow W - \eta \frac{\partial L}{\partial W}
$$

($\eta$는 learning rate)

## 주의할 점(Caveats)
- **stride/padding/dilation**이 있어도 “여러 위치에서 합산된다” 구조는 변하지 않는다(인덱싱만 달라짐).
- **gradient accumulation** 같은 설정을 쓰면 “매 batch마다 업데이트”가 아니라 “여러 batch 후 1회 업데이트”가 될 수 있다.
- “어떤 커널이 무엇을 학습했는지”는 feature visualization, Grad-CAM 등으로 확인하는 편이 안전하다.

## 한 줄 요약(Summary)
Conv2D의 kernel 값은 학습되는 가중치이며, weight sharing 때문에 커널 한 칸의 gradient는 배치/공간 전체에서 나온 기여가 합산되어 업데이트된다.

