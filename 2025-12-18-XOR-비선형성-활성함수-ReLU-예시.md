# XOR로 보는 비선형성(활성함수)의 필요성과 ReLU 예시

> Type: Concept

## 상황/배경(Context)
활성함수가 “비선형성을 추가한다”는 말이 추상적으로 느껴졌다. 왜 활성함수가 없으면 네트워크가 선형으로 붕괴되는지, 그리고 XOR 같은 문제에서 비선형이 어떻게 해결책이 되는지 예시로 정리했다.

---

## 정의(Definition)
활성함수(activation function)는 각 layer의 선형 변환(`Wx+b`) 뒤에 붙는 **비선형 함수**다. 이 비선형이 있어야 여러 layer를 쌓았을 때 네트워크 전체가 단순한 선형 모델로 합쳐지지 않고, 복잡한 결정경계를 표현할 수 있다.

## 핵심 아이디어(Key Ideas)
- 활성함수 없이 선형 layer만 쌓으면, 합성 결과가 다시 `W'x+b'` 꼴로 정리되어 **표현력이 선형 모델 수준**으로 제한된다.
- XOR은 선형 분리(linear separable)가 불가능해서, 선형 모델은 완벽히 풀 수 없다.
- 비선형 활성함수(ReLU 등)를 넣으면 공간을 “조각(piecewise)” 내서, XOR 같은 패턴을 표현할 수 있다.

## 역사/배경(Timeline/Why)
- 단층 퍼셉트론/선형 모델의 한계(XOR 등)가 반복적으로 드러났고, 이를 넘기려면 “여러 층 + 비선형”이 필요했다.
- 역전파가 보급되면서(미분 기반 학습) 미분 가능한 비선형(sigmoid/tanh)이 널리 쓰였지만, 깊어지면 vanishing gradient 문제가 컸다.
- 이후 ReLU 계열, 초기화/정규화, 잔차 연결 등이 결합되며 깊은 비선형 모델이 실용화됐다.

## 원리/수식(Principle/Math)
### 활성함수 없으면 왜 선형으로 붕괴하나?
선형/아핀 layer만 2개 쌓으면:

- 1층: $h = W_1 x + b_1$
- 2층: $y = W_2 h + b_2$

대입하면:

$$
y = W_2(W_1 x + b_1) + b_2 = (W_2 W_1)x + (W_2 b_1 + b_2)
$$

즉 다시 $y = W' x + b'$ 꼴이 되어, 여러 층을 쌓아도 결국 하나의 선형 모델과 같다.

## 예시(Examples)
### XOR 진리표
| x1 | x2 | XOR(y) |
|---:|---:|:------:|
| 0  | 0  |   0    |
| 0  | 1  |   1    |
| 1  | 0  |   1    |
| 1  | 1  |   0    |

### ReLU 2개 은닉 뉴런으로 XOR 만들기(2-2-1)
ReLU를 $ReLU(t)=\max(0,t)$라고 하자.

은닉층(2개):
- $h_1 = ReLU(x_1 - x_2)$
- $h_2 = ReLU(x_2 - x_1)$

출력(선형):
- $y_{score} = h_1 + h_2$

예측 규칙(이 XOR 도메인에선 충분):
- $y_{pred} = 1 \; \text{if } y_{score} > 0 \; \text{else } 0$

#### 4개 입력을 전부 계산해서 검증
1) 입력 $(0,0)$
- $h_1 = ReLU(0-0)=0$
- $h_2 = ReLU(0-0)=0$
- $y_{score}=0$ → $y_{pred}=0$ (정답 0)

2) 입력 $(0,1)$
- $h_1 = ReLU(0-1)=0$
- $h_2 = ReLU(1-0)=1$
- $y_{score}=1$ → $y_{pred}=1$ (정답 1)

3) 입력 $(1,0)$
- $h_1 = ReLU(1-0)=1$
- $h_2 = ReLU(0-1)=0$
- $y_{score}=1$ → $y_{pred}=1$ (정답 1)

4) 입력 $(1,1)$
- $h_1 = ReLU(1-1)=0$
- $h_2 = ReLU(1-1)=0$
- $y_{score}=0$ → $y_{pred}=0$ (정답 0)

이 예시에서 중요한 점은, 두 ReLU가 `x1>x2`, `x2>x1` 영역에서 각각 켜지면서 공간을 조각내고(piecewise), 그 결과 XOR 패턴을 표현할 수 있게 된다는 것이다.

## 주의할 점(Caveats)
- 위 예시는 “표현 가능하다”를 보여주는 구성이다. 실제 학습에서는 loss/optimizer/초기화에 따라 파라미터를 학습해 이런 형태를 찾는다.
- 출력 스코어를 확률로 해석하려면 마지막에 sigmoid/softmax 같은 함수를 붙이고 적절한 loss(예: cross entropy)를 사용한다.

## 한 줄 요약(Summary)
활성함수는 선형 layer의 반복을 “비선형 함수의 합성”으로 바꿔 표현력을 크게 늘리고, ReLU 같은 비선형을 쓰면 XOR처럼 선형 분리 불가능한 패턴도 표현/해결할 수 있다.
