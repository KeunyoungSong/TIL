# Keras `fit()`: 옵션 정리

> Type: Concept

## 상황/배경(Context)
강의/블로그/버전마다 `model.fit()` 시그니처가 조금씩 달라서 헷갈린다.  
특히 입력 형태(`x`, `y`), validation 지정 방식, `steps_per_epoch` 같은 옵션이 어디서 필요한지 한 번에 정리한다.

---

## 정의(Definition)
`model.fit(...)`은 주어진 학습 데이터를 여러 epoch 동안 반복하며, loss/metric을 기반으로 모델 파라미터를 업데이트(훈련)하는 Keras의 표준 학습 API다.

## 핵심 아이디어(Key Ideas)
- `fit()` 옵션의 대부분은 “데이터가 어떤 형태냐(배열 vs dataset/generator)”에 따라 의미/동작이 갈린다.
- validation은 `validation_data`가 가장 명시적이고, `validation_split`은 간편하지만 제약이 있다.
- `steps_per_epoch`, `validation_steps`는 **dataset이 무한 반복**이거나, epoch을 “일부 step만” 돌리고 싶을 때 중요하다.

## 도식(Diagram)
```
train (x, y[, w])  --fit-->  epoch 반복
                         ├─ batch 단위로 forward/backward
                         └─ (옵션) epoch 끝에 validation 평가
```

## 원리/수식(Principle/Math)
옵션 자체는 수식보다 “루프 제어”에 가깝다.
- 한 epoch = (보통) 학습 데이터 전체를 한 번 소비
- 한 step = 배치 1개 처리(gradient update 1회)

`steps_per_epoch`를 지정하면 “epoch=데이터 전체”라는 정의가 바뀌고,
`validation_steps`를 지정하면 validation도 “일부 배치만 평가”할 수 있다.

## 예시(Examples)
### 1) NumPy/텐서 입력(배치 크기 직접 지정)
```python
history = model.fit(
    x_train, y_train,
    batch_size=64,
    epochs=20,
    validation_split=0.2,
    shuffle=True,
)
```

### 2) `tf.data.Dataset` 입력(배치는 dataset이 책임)
```python
history = model.fit(
    train_ds,
    epochs=20,
    validation_data=val_ds,
)
```

### 3) epoch / batch / step 직관(가중치 업데이트 포함)
데이터가 6만 장($N=60000$)이고 `batch_size=B`일 때(일반적인 mini-batch 학습 기준):
- **epoch**: 전체 데이터를 1번 “소비”하는 단위. `epochs=E`면 (보통) 전체 데이터를 E번 반복해서 본다.
- **batch_size(batch)**: 한 step에서 한 번에 처리하는 샘플 수. GPU 메모리의 영향을 크게 받지만, 속도/일반화/노이즈에도 영향을 준다.
- **step(iteration)**: 배치 1개를 처리하는 단위.
  - 보통 `steps_per_epoch ≈ ceil(N / B)` (마지막 배치가 남는 경우를 포함할 때)
  - 예: `N=60000`, `B=128`이면 `ceil(60000/128)=469` step이 1 epoch를 이룬다.
- **가중치 업데이트**: 표준 SGD/Adam 학습에서는 **매 step(=매 batch)**마다 gradient를 계산하고 가중치를 업데이트한다.
  - 예외로 gradient accumulation 같은 설정을 쓰면 “여러 batch 후 1회 업데이트”도 가능하지만, 기본 `fit()` 흐름은 step마다 업데이트로 이해하면 된다.

## 옵션 정리(Parameters)
### 데이터/타깃
- `x`: 입력 데이터.
  - 배열/텐서, 여러 입력이면 `list` 또는 “입력 이름 → 값”의 `dict`
  - `tf.data.Dataset`/generator/`PyDataset`도 가능(보통 `(inputs, targets[, sample_weights])`를 yield)
- `y`: 정답(target). `x`가 dataset/generator류면 보통 `y`를 따로 주지 않는다(이미 `x`에서 함께 나오기 때문).

### 학습 루프(배치/epoch/로그)
- `batch_size`: 배치 크기. 배열/텐서 입력일 때만 의미가 크다. dataset/generator류에선 보통 지정하지 않는다.
- `epochs`: 학습을 몇 epoch까지 진행할지(종료 epoch). `initial_epoch`와 함께 “어디서부터 어디까지”의 의미로 이해한다.
- `initial_epoch`: 이어 학습(resume)할 때 시작 epoch.
- `verbose`: `0`(무출력), `1`(progress bar), `2`(epoch당 1줄), `"auto"`.
- `callbacks`: 학습 중 훅. 예: `ModelCheckpoint`, `EarlyStopping`, `ReduceLROnPlateau`.

### Validation
- `validation_split`: 학습 데이터 일부를 validation으로 떼어낸다(0~1).
  - 보통 **셔플 전 마지막 구간**에서 떼어낸다.
  - `x`가 dataset/generator류면 지원되지 않는 경우가 많다.
- `validation_data`: validation 데이터를 직접 지정한다.
  - `(x_val, y_val)` 또는 `(x_val, y_val, w_val)` 또는 dataset/generator
  - `validation_split`보다 우선한다.
- `validation_freq`: validation을 몇 epoch마다 수행할지(예: `2`면 2 epoch마다).
- `validation_steps`: validation이 dataset/generator일 때 몇 step 평가할지. `None`이면 보통 데이터가 소진될 때까지.
- `validation_batch_size`: validation 배치 크기(배열/텐서 validation일 때 의미가 큼). 기본은 `batch_size`.

### 셔플/가중치(불균형/샘플 가중)
- `shuffle`: 매 epoch마다 학습 데이터를 섞을지. dataset/generator류에선 무시될 수 있다(파이프라인 쪽에서 셔플).
- `class_weight`: “클래스 인덱스 → 가중치” 딕셔너리. 클래스 불균형에서 loss를 더 크게/작게 반영하고 싶을 때 사용(훈련에만 적용).
- `sample_weight`: 샘플별 가중치(훈련에만 적용).
  - 1D(샘플별) 또는 시계열이면 2D(샘플×타임스텝)
  - dataset/generator류면 보통 `(inputs, targets, sample_weights)`로 함께 제공

### Step 제어(특히 dataset에서 중요)
- `steps_per_epoch`: epoch당 학습 step 수.
  - 무한 반복 dataset(`repeat()`)이면 필수인 경우가 많다.
  - 지정하면 “epoch=전체 데이터”가 아니라 “epoch=지정한 step 수”가 된다.
- `validation_steps`: validation도 동일한 개념으로 step 수를 제한한다(무한 반복 validation이면 지정 필요).

## 주의할 점(Caveats)
- **`validation_split`은 편하지만 예측 가능성이 낮다**: “마지막 구간을 떼는” 동작 때문에 데이터가 시간순/정렬된 상태면 validation이 치우칠 수 있다. 가능하면 `validation_data`를 명시적으로 두는 편이 안전하다.
- **dataset 입력이면 `batch_size`, `shuffle` 의미가 달라진다**: 배치/셔플은 보통 `train_ds.batch(...).shuffle(...)` 쪽에서 제어한다.
- **`sample_weight`는 metric에 기본 적용이 아닐 수 있다**: metric까지 가중을 반영하려면 `compile(weighted_metrics=...)`를 쓰는 구성이 필요하다.
- **강의와 시그니처가 다른 이유**: Keras/TF 버전에 따라 `workers`, `use_multiprocessing`, `max_queue_size` 같은 인자가 있었거나(주로 generator/`Sequence` 최적화), 최신 API에선 빠진 경우가 있다. 핵심은 “입력 파이프라인을 어디서 병렬화하느냐”로 이동했다는 점이다.

## 한 줄 요약(Summary)
`fit()`은 “학습 루프를 돌리는 API”이고, 옵션은 대부분 **입력 데이터 형태(배열 vs dataset/generator)**에 따라 동작/필요성이 갈린다.

## 참고(Links)
- https://keras.io/api/models/model_training_apis/#fit-method
