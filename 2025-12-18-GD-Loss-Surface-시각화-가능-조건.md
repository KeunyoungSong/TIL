# GD의 Loss Surface 3D 시각화: 언제 가능하고 무엇을 단순화한 건가

> Type: Concept

## 상황/배경(Context)
옵티마이저를 설명할 때 자주 나오는 “골짜기 모양 3D 그래프(Loss Surface)”를 보며, 실제 딥러닝의 GD(Gradient Descent)도 저렇게 3차원 표면 위를 이동한다고 말할 수 있는지, 그리고 “입력 차원”, “히든 레이어”, “파라미터 개수”가 시각화 가능 여부와 어떻게 연결되는지 헷갈렸다.

---

## 정의(Definition)
Loss Surface는 파라미터(가중치/바이어스) 공간에서 loss 값을 함수로 본 것이다.

- 파라미터가 `θ`일 때: `L = L(θ)`
- 파라미터가 2개(`θ1`, `θ2`)일 때만 3D로 그릴 수 있다:
  - x축: `θ1`
  - y축: `θ2`
  - z축: `L(θ1, θ2)`

## 핵심 아이디어(Key Ideas)
- 3D로 그릴 수 있는 조건은 “입력 차원”이 아니라 **학습 파라미터 차원(개수)**이다.
- 입력 차원은 데이터의 모양(shape)이고, Loss Surface의 축이 아니다.
- 히든 레이어 유무는 “표현력”과는 관련이 있지만, 시각화 관점에서는 거의 항상 **파라미터 수를 늘려** 시각화를 불가능하게 만든다.
- 강의에서 보는 3D 지형은 “거짓”이 아니라, 고차원 loss landscape의 성질을 설명하기 위한 **추상화**다.

## 기호 정리(Symbols)
- `x`: 입력(데이터), 한 샘플의 특성 벡터/텐서
- `θ`: 학습 파라미터(가중치/바이어스)
- `L(θ)`: loss(손실 함수)
- `N`: 샘플 수
- `η`: learning rate

## 원리/수식(Principle/Math)
### 1) Loss Surface의 차원은 파라미터로 결정된다
파라미터가 `n`개면 loss는 `n`차원 입력을 받는 함수다.

$$
L = L(\theta_1, \theta_2, \ldots, \theta_n)
$$

이를 “표면”으로 그리려면 축이 필요하다.

- `n=2`이면: `(θ1, θ2)` 평면 위의 높이 `L` → 3D로 가능
- `n≥3`이면: `(θ1, θ2, θ3, ...)`는 3차원 초과 → 인간이 직접 시각화 불가

### 2) 입력 차원과 파라미터 차원은 독립이다
Loss는 데이터 전체에 대해 평균/합 형태로 정의된다.

$$
L(\theta_1,\theta_2)
=
\frac{1}{N}\sum_{i=1}^{N}\ell\big(f(x_i;\theta_1,\theta_2), y_i\big)
$$

여기서 `x_i`의 차원(예: `x_i ∈ R^{100}`)이 크더라도, **학습되는 변수**가 `θ1, θ2` 두 개뿐이면 `L(θ1, θ2)`로 남는다. 즉 “입력 차원 100”은 축 개수를 늘리지 않는다.

### 3) GD는 파라미터 공간에서 움직인다
Gradient Descent는 파라미터를 업데이트한다.

$$
(\theta_1, \theta_2)\leftarrow(\theta_1,\theta_2)-\eta\nabla_\theta L(\theta_1,\theta_2)
$$

여기서 “움직이는 좌표”가 `θ`이고, 입력 `x`는 이미 주어진 상수 데이터(고정된 관측치)다.

## 예시(Examples)
### 예시 1) 입력 차원 2여도 3D가 안 될 수 있다
입력 `x ∈ R^2`이고 선형 모델이

$$
y=w_1x_1+w_2x_2+b
$$

이면 학습 파라미터는 `(w1, w2, b)`로 3개다. 따라서 `L(w1,w2,b)`는 3차원 입력 함수이고, 3D surface로 “완전한 형태”를 그릴 수 없다.

### 예시 2) 입력 차원 100이어도 3D가 될 수 있다(특수 케이스)
입력 `x ∈ R^{100}`이라도, 학습 파라미터가 두 개뿐이면 된다.

$$
y = a\cdot f(x) + b
$$

여기서 `f(x)`가 학습되지 않는 고정 함수(특징 추출)이고, 학습 파라미터가 `(a,b)` 두 개뿐이면 loss는 `L(a,b)`가 되고 3D로 그릴 수 있다.

### 예시 3) 히든 레이어가 있으면 보통 즉시 불가능해진다
입력 1차원이라도, 히든 1개 뉴런을 가진 아주 작은 네트워크를 보자.

- 입력→히든: `w1`, `b1`
- 히든→출력: `w2`, `b2`

학습 파라미터가 4개라서 `L(w1,b1,w2,b2)`가 되고, 3D surface로는 그릴 수 없다.

## 실제로 “그려보는” 타협(연구/교육)
전체 loss landscape를 그리는 대신, 2차원 단면(slice)이나 투영(projection)을 그린다.

- **slice**: 두 파라미터(또는 두 방향)만 움직이고 나머지는 고정  
  - `θ + αv1 + βv2` 형태로 샘플링해서 `L(α,β)`를 그림
- **projection**: 고차원 경로/파라미터를 PCA 같은 방법으로 2D로 투영

이들은 “전체 지형”이 아니라 “일부 단면/요약”이라는 점이 중요하다.

## 주의할 점(Caveats)
- 강의의 3D 그림은 개념 설명에 유용하지만, 실제 딥러닝 최적화는 수백만~수억 파라미터 공간에서 일어난다.
- 고차원에서는 local minimum보다 saddle point(기울기는 작지만 최소가 아닌 지점) 같은 현상이 중요한 경우가 많고, 이를 설명하기 위해 2D/3D로 단순화해 보여주기도 한다.

## 한 줄 요약(Summary)
GD를 3D 지형으로 “완전하게” 그릴 수 있는 조건은 입력 차원이 아니라 **학습 파라미터가 정확히 2개일 때**이며, 실제 딥러닝에서는 slice/projection 같은 방법으로 일부만 시각화한다.

## 용어(Glossary)
- **Loss Surface/Loss Landscape**: 파라미터 공간에서의 loss 함수 지형
- **parameter dimension**: 학습되는 스칼라 파라미터 개수
- **input dimension**: 한 샘플의 특성 수(MLP에서는 보통 입력 노드 수와 동일)
- **slice**: 고차원 함수의 일부 단면만 고정하고 보는 시각화
- **projection**: 고차원 데이터를 저차원으로 투영해 보는 시각화
